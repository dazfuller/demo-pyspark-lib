# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

variables:
- group: databricks-deployment
- name: python-version
  value: '3.7'

stages:
- stage: Build
  jobs:
  - job: sparklib
    displayName: 'Build demo library'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
      - task: UsePythonVersion@0
        inputs:
          versionSpec: $(python-version)
          addToPath: true
          architecture: 'x64'
        displayName: 'Set up Python 3'

      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            python3 -m pip install --upgrade pip
            python3 -m pip install -r requirements.txt
        displayName: 'Install dependencies'
      
      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            python3 -m coverage run --branch -m xmlrunner -o test-results discover -v -s ./tests -p test_*.py
            python3 -m coverage xml
            python3 -m coverage html
        displayName: 'Run tests and produce code coverage'
      
      - task: PublishTestResults@2
        inputs:
          testResultsFormat: 'JUnit'
          testResultsFiles: 'test-results/TEST-*.xml'
          mergeTestResults: true
          failTaskOnFailedTests: true
          testRunTitle: 'Test results for build id $(Build.BuildId)'
        displayName: 'Publish unit tests'
      
      - task: PublishCodeCoverageResults@1
        inputs:
          codeCoverageTool: 'Cobertura'
          summaryFileLocation: 'coverage.xml'
          reportDirectory: 'htmlcov'
        displayName: 'Publish code coverage results'

      - task: PowerShell@2
        inputs:
          targetType: inline
          pwsh: true
          script: |
            ./set-version -BuildId $(Build.BuildId)
            python setup.py bdist_wheel
        displayName: 'Build the wheel file'
      
      - task: PublishPipelineArtifact@1
        inputs:
          targetPath: 'dist'
          artifactName: LibWheel
        displayName: 'Publish artifact to pipeline'

- stage: DeployTest
  displayName: 'Deploy to test'
  dependsOn: Build
  condition: succeeded()
  jobs:
  - job: test
    displayName: 'Deploy library to test'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
      - task: DownloadPipelineArtifact@2
        inputs:
          artifactName: LibWheel
          patterns: '**/*.whl'
          path: $(Build.SourcesDirectory)/dist
        displayName: 'Download artifact from pipeline'
      
      - task: UsePythonVersion@0
        inputs:
          versionSpec: $(python-version)
          addToPath: true
          architecture: 'x64'
        displayName: 'Set up Python 3'
      
      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            python3 -m pip install databricks-cli
        displayName: 'Install Databricks CLI'
      
      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            $WheelFile = Get-ChildItem -Path ./dist -Filter *.whl | Select-Object -First 1
            databricks fs mkdirs dbfs:/FileStore/libraries
            databricks fs cp $WheelFile.FullName dbfs:/FileStore/libraries
        env:
          DATABRICKS_HOST: $(test-databricks-host)
          DATABRICKS_TOKEN: $(test-databricks-pat)
        displayName: 'Publish wheel file to test workspace'
  
- stage: DeployProd
  displayName: 'Deploy to production'
  dependsOn: DeployTest
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  jobs:
  - job: prod
    displayName: 'Deploy library to production'
    pool:
      vmImage: 'ubuntu-latest'

    steps:
      - task: DownloadPipelineArtifact@2
        inputs:
          artifactName: LibWheel
          patterns: '**/*.whl'
          path: $(Build.SourcesDirectory)/dist
        displayName: 'Download artifact from pipeline'
      
      - task: UsePythonVersion@0
        inputs:
          versionSpec: $(python-version)
          addToPath: true
          architecture: 'x64'
        displayName: 'Set up Python 3'
      
      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            python3 -m pip install databricks-cli
        displayName: 'Install Databricks CLI'
      
      - task: PowerShell@2
        inputs:
          targetType: 'inline'
          pwsh: true
          script: |
            $WheelFile = Get-ChildItem -Path ./dist -Filter *.whl | Select-Object -First 1
            databricks fs mkdirs dbfs:/FileStore/libraries
            databricks fs cp $WheelFile.FullName dbfs:/FileStore/libraries/dazspark-py3.whl
        env:
          DATABRICKS_HOST: $(test-databricks-host)
          DATABRICKS_TOKEN: $(test-databricks-pat)
        displayName: 'Publish wheel file to test workspace'
